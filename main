import os 

 

import numpy as np 

import cv2 

 

import matplotlib.pyplot as plt 

import matplotlib.image as mpimg 

 

from PIL import Image, ImageFile 

ImageFile.LOAD_TRUNCATED_IMAGES = True 

from IPython.display import display 

 

import torch 

import torch.nn as nn 

 

from torch.utils.data import DataLoader 

import torch.nn.functional as F 

from torchvision import datasets, transforms, models 

from torch.optim.lr_scheduler import StepLR 

from torchsummary import summary 

from tqdm import tqdm  

 

data_path = "C:\\Users\\noah_kuskye\\Downloads\\New folder\\Adv._CS_Topics_AI_Project\\AI_Data" 

 

class_name = ['tail','no_tail'] 

def get_list_files(dirName): 

    ''' 

    input - directory location 

    output - list the files in the directory 

    ''' 

    files_list = os.listdir(dirName) 

    return files_list 

 

files_list_tail_train = get_list_files(data_path+'/train/'+class_name[0]) 

files_list_no_tail_train = get_list_files(data_path+'/train/'+class_name[1]) 

 

files_list_tail_test = get_list_files(data_path+'/test/'+class_name[0]) 

files_list_no_tail_test = get_list_files(data_path+'/test/'+class_name[1]) 

 

print("Number of train samples in Normal category {}".format(len(files_list_tail_train))) 

print("Number of train samples in Pneumonia category {}".format(len(files_list_no_tail_train))) 

 

print("Number of test samples in Normal category {}".format(len(files_list_tail_test))) 

print("Number of test samples in Pneumonia category {}".format(len(files_list_no_tail_test))) 

 

train_transform = transforms.Compose([ 

    transforms.Resize(224), 

    transforms.CenterCrop(224), 

    transforms.ToTensor(), 

    transforms.Normalize([0.485, 0.456, 0.406], 

                          [0.229, 0.224, 0.225])]) 

test_transform = transforms.Compose([ 

    transforms.Resize(224), 

    transforms.CenterCrop(224), 

    transforms.ToTensor(), 

    transforms.Normalize([0.485, 0.456, 0.406], 

                          [0.229, 0.224, 0.225])]) 

 

train_data = datasets.ImageFolder(os.path.join(data_path, 'train'), transform= train_transform) 

test_data = datasets.ImageFolder(os.path.join(data_path, 'test'), transform= test_transform) 

 

train_loader = DataLoader(train_data, 

                          batch_size= 16, shuffle= True, pin_memory= True) 

test_loader = DataLoader(test_data, 

                         batch_size= 1, shuffle= False, pin_memory= True) 

 

class_names = train_data.classes 

 

print(class_names) 

print(f'Number of train images: {len(train_data)}') 

print(f'Number of test images: {len(test_data)}') 

 

class Net(nn.Module): 

    def __init__(self): 

        super(Net, self).__init__() 

        # Input Block 

        self.convblock1 = nn.Sequential( 

            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3), 

                      padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(4) 

        ) 

        self.pool11 = nn.MaxPool2d(2, 2) 

 

        # CONVOLUTION BLOCK 

        self.convblock2 = nn.Sequential( 

            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), 

                      padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(16) 

        ) 

 

        # TRANSITION BLOCK 

 

        self.pool22 = nn.MaxPool2d(2, 2) 

 

        self.convblock3 = nn.Sequential( 

            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False), 

            #nn.BatchNorm2d(10), 

            nn.ReLU() 

        ) 

        self.pool33 = nn.MaxPool2d(2, 2) 

 

        # CONVOLUTION BLOCK 

        self.convblock4 = nn.Sequential( 

            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(10) 

        ) 

 

        self.convblock5 = nn.Sequential( 

            nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding=0, bias=False), 

            #nn.BatchNorm2d(32), 

            nn.ReLU(), 

 

        ) 

 

        self.convblock6 = nn.Sequential( 

            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(10), 

 

        ) 

 

        self.convblock7 = nn.Sequential( 

            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(10) 

 

        ) 

 

        self.convblock8 = nn.Sequential( 

            nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding=0, bias=False), 

            #nn.BatchNorm2d(32), 

            nn.ReLU() 

        ) 

        self.convblock9 = nn.Sequential( 

            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(10), 

 

        ) 

 

        self.convblock10 = nn.Sequential( 

            nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(14), 

 

        ) 

 

        self.convblock11 = nn.Sequential( 

            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=False), 

            nn.ReLU(), 

            #nn.BatchNorm2d(16), 

 

        ) 

 

        # OUTPUT BLOCK 

        self.gap = nn.Sequential( 

            nn.AvgPool2d(kernel_size=4) 

        ) 

 

        self.convblockout = nn.Sequential( 

              nn.Conv2d(in_channels=16, out_channels=2, kernel_size=(4, 4), padding=0, bias=False), 

 

        ) 

 

    def forward(self, x): 

        x = self.convblock1(x) 

        x = self.pool11(x) 

        x = self.convblock2(x) 

        x = self.pool22(x) 

        x = self.convblock3(x) 

        x = self.pool33(x) 

        x = self.convblock4(x) 

        x = self.convblock5(x) 

        x = self.convblock6(x) 

        x = self.convblock7(x) 

        x = self.convblock8(x) 

        x = self.convblock9(x) 

        x = self.convblock10(x) 

        x = self.convblock11(x) 

        x = self.gap(x) 

        x = self.convblockout(x) 

 

        x = x.view(-1, 2) 

        return F.log_softmax(x, dim=-1) 

     

use_cuda = torch.cuda.is_available() 

device = torch.device("cuda" if use_cuda else "cpu") 

print("Available processor {}".format(device)) 

model = Net().to(device) 

summary(model, input_size=(3, 224, 224)) 

 

train_losses = [] 

test_losses = [] 

train_acc = [] 

test_acc = [] 

 

def train(model, device, train_loader, optimizer, epoch): 

    model.train() 

    pbar = tqdm(train_loader) 

    correct = 0 

    processed = 0 

    for batch_idx, (data, target) in enumerate(pbar): 

        # get data 

        data, target = data.to(device), target.to(device) 

 

        # Initialization of gradient 

        optimizer.zero_grad() 

        # In PyTorch, gradient is accumulated over backprop and even though thats used in RNN generally not used in CNN 

        # or specific requirements 

        ## prediction on data 

        y_pred = model(data) 

 

        # Calculating loss given the prediction 

        loss = F.nll_loss(y_pred, target) 

        train_losses.append(loss) 

 

        # Backprop 

        loss.backward() 

        optimizer.step() 

        # get the index of the log-probability corresponding to the max value 

        pred = y_pred.argmax(dim=1, keepdim=True) 

        correct += pred.eq(target.view_as(pred)).sum().item() 

        processed += len(data) 

 

        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}') 

        train_acc.append(100*correct/processed) 

 

def test(model, device, test_loader): 

    model.eval() 

    test_loss = 0 

    correct = 0 

    with torch.no_grad(): 

        for data, target in test_loader: 

            data, target = data.to(device), target.to(device) 

            output = model(data) 

            test_loss += F.nll_loss(output, target, reduction='sum').item() 

            pred = output.argmax(dim=1, keepdim=True) 

            correct += pred.eq(target.view_as(pred)).sum().item() 

 

    test_loss /= len(test_loader.dataset) 

    test_losses.append(test_loss) 

 

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format( 

            test_loss, correct, len(test_loader.dataset), 

            100. * correct / len(test_loader.dataset))) 

 

    test_acc.append(100. * correct / len(test_loader.dataset)) 

 

model =  Net().to(device) 

optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 

scheduler = StepLR(optimizer, step_size=6, gamma=0.5) 

 

EPOCHS = 15 

for epoch in range(EPOCHS): 

    print("EPOCH:", epoch) 

    train(model, device, train_loader, optimizer, epoch) 

    scheduler.step() 

    print('current Learning Rate: ', optimizer.state_dict()["param_groups"][0]["lr"]) 

    test(model, device, test_loader) 

 
